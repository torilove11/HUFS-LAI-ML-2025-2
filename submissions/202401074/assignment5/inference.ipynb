{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# inference.ipynb (500íšŒ í•™ìŠµ ë²„ì „)\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 1. ë°ì´í„° ë° í´ë” ì¤€ë¹„\n",
        "os.makedirs('data', exist_ok=True)\n",
        "os.makedirs('model_artifacts', exist_ok=True)\n",
        "\n",
        "# 2. ë°ì´í„° ìƒì„± (500ê°œ ì´ìƒ)\n",
        "formal_sentences = [\n",
        "    \"êµìˆ˜ë‹˜, ì´ë²ˆ ê³¼ì œ ì œì¶œ ê¸°í•œì„ ì—°ìž¥í•´ì£¼ì‹¤ ìˆ˜ ìžˆìœ¼ì‹ ê°€ìš”?\",\n",
        "    \"ì•ˆë…•í•˜ì„¸ìš”, ìˆ˜ì—… ë‚´ìš© ì¤‘ ì§ˆë¬¸ì´ ìžˆì–´ ë©”ì¼ ë“œë¦½ë‹ˆë‹¤.\",\n",
        "    \"ì¶œì„ ì¸ì • ê´€ë ¨í•˜ì—¬ ì¦ë¹™ ì„œë¥˜ë¥¼ ì œì¶œí•˜ê³ ìž í•©ë‹ˆë‹¤.\",\n",
        "    \"í˜¹ì‹œ ìƒë‹´ ê°€ëŠ¥í•˜ì‹  ì‹œê°„ì´ ì–¸ì œì´ì‹ ì§€ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.\",\n",
        "    \"ì¡¸ì—… ìš”ê±´ê³¼ ê´€ë ¨í•˜ì—¬ ë©´ë‹´ì„ ìš”ì²­ë“œë ¤ë„ ë ê¹Œìš”?\",\n",
        "    \"ì£„ì†¡í•˜ì§€ë§Œ ê°œì¸ì ì¸ ì‚¬ì •ìœ¼ë¡œ ìˆ˜ì—… ì°¸ì„ì´ ì–´ë µìŠµë‹ˆë‹¤.\",\n",
        "    \"íŒ€ í”„ë¡œì íŠ¸ ì¡° íŽ¸ì„± ë¬¸ì œë¡œ ë¬¸ì˜ë“œë¦½ë‹ˆë‹¤.\"\n",
        "] * 40\n",
        "\n",
        "informal_sentences = [\n",
        "    \"ì•¼ ì˜¤ëŠ˜ ì ì‹¬ ë­ ë¨¹ì„ëž˜?\",\n",
        "    \"ì•„ ì§„ì§œ ê³¼ì œ ê°œë§Žë„¤ ã…‹ã…‹ã…‹\",\n",
        "    \"ë¡¤ í•˜ëŸ¬ ê°ˆ ì‚¬ëžŒ êµ¬í•¨\",\n",
        "    \"ë„ˆ ì–´ì œ ìˆ˜ì—… ë“¤ì—ˆëƒ? í•„ê¸° ì¢€ ë³´ì—¬ì¤˜\",\n",
        "    \"ì˜¤ëŠ˜ ìˆ  í•œìž” ã„±ã„±?\",\n",
        "    \"ì´ë²ˆ ì£¼ë§ì— ì‹œê°„ ë¨?\",\n",
        "    \"ì•„ êµìˆ˜ë‹˜ ì§„ë„ ë„ˆë¬´ ë¹¨ë¦¬ ë‚˜ê°€ì‹œëŠ” ê±° ì•„ë‹ˆëƒ\"\n",
        "] * 40\n",
        "\n",
        "data = [{'text': t, 'label': 1} for t in formal_sentences] + \\\n",
        "       [{'text': t, 'label': 0} for t in informal_sentences]\n",
        "\n",
        "df = pd.DataFrame(data).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# 3. ëª¨ë¸ í•™ìŠµ (ê³µë¶€ ì‹œìž‘)\n",
        "vectorizer = CountVectorizer(max_features=2000)\n",
        "X = vectorizer.fit_transform(df['text']).toarray()\n",
        "y = torch.FloatTensor(df['label'].values).unsqueeze(1)\n",
        "\n",
        "class ExtendedClassifier(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ExtendedClassifier, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.layer3 = nn.Linear(32, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "model = ExtendedClassifier(X.shape[1])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "model.train()\n",
        "inputs = torch.FloatTensor(X)\n",
        "\n",
        "print(\"ðŸš€ ëª¨ë¸ì´ 500íšŒ í•™ìŠµì„ ì‹œìž‘í•©ë‹ˆë‹¤! (ìž ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...)\")\n",
        "\n",
        "# [ìˆ˜ì •ë¨] 500íšŒ ë°˜ë³µ í•™ìŠµ\n",
        "for epoch in range(500):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100ë²ˆë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f\"í•™ìŠµ ì§„í–‰ë¥ : {epoch+1}/500 (Loss: {loss.item():.6f})\")\n",
        "\n",
        "torch.save(model.state_dict(), 'model_artifacts/model_weights.pth')\n",
        "joblib.dump(vectorizer, 'model_artifacts/vectorizer.pkl')\n",
        "print(\"ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\\n\")\n",
        "\n",
        "# 4. ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸\n",
        "def predict_politeness(sentences):\n",
        "    model.eval()\n",
        "    vectorizer = joblib.load('model_artifacts/vectorizer.pkl')\n",
        "\n",
        "    test_inputs = vectorizer.transform(sentences).toarray()\n",
        "    test_tensor = torch.FloatTensor(test_inputs)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(test_tensor)\n",
        "        probs = outputs.numpy().flatten()\n",
        "        preds = (probs > 0.5).astype(int)\n",
        "\n",
        "    print(f\"{'ìž…ë ¥ ë¬¸ìž¥':<40} | {'íŒë‹¨ ê²°ê³¼ (Label)'}\")\n",
        "    print(\"-\" * 80)\n",
        "    for text, pred in zip(sentences, preds):\n",
        "        if pred == 1:\n",
        "            result = \"Label 1 : ê²©ì‹ (êµìˆ˜ë‹˜ìš©)\"\n",
        "        else:\n",
        "            result = \"Label 0 : ë¹„ê²©ì‹ (ì¹œêµ¬ìš©)\"\n",
        "\n",
        "        print(f\"{text:<40} | {result}\")\n",
        "\n",
        "my_sentences = [\n",
        "    \"êµìˆ˜ë‹˜ ì•ˆë…•í•˜ì„¸ìš”, ê³¼ì œ ê´€ë ¨ ë¬¸ì˜ë“œë¦½ë‹ˆë‹¤.\",\n",
        "    \"ì•¼ ë„ˆ ì˜¤ëŠ˜ í”¼ë°© ê°ˆëž˜?\",\n",
        "    \"í˜¹ì‹œ ëž©ë¯¸íŒ… ì¼ì •ì´ ë³€ê²½ë˜ì—ˆë‚˜ìš”?\",\n",
        "    \"ì•„ ì§„ì§œ ê°œí”¼ê³¤í•˜ë‹¤ ã…‹ã…‹ã…‹\",\n",
        "    \"ì£„ì†¡í•˜ì§€ë§Œ ì¶œì„ ì¸ì • ì„œë¥˜ ì œì¶œì´ ëŠ¦ì–´ì§ˆ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\"\n",
        "]\n",
        "\n",
        "predict_politeness(my_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0GI43w_W4iR",
        "outputId": "bd04b12f-365c-421d-8c71-d2500fbeef56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ ëª¨ë¸ì´ 500íšŒ í•™ìŠµì„ ì‹œìž‘í•©ë‹ˆë‹¤! (ìž ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...)\n",
            "í•™ìŠµ ì§„í–‰ë¥ : 100/500 (Loss: 0.000000)\n",
            "í•™ìŠµ ì§„í–‰ë¥ : 200/500 (Loss: 0.000000)\n",
            "í•™ìŠµ ì§„í–‰ë¥ : 300/500 (Loss: 0.000000)\n",
            "í•™ìŠµ ì§„í–‰ë¥ : 400/500 (Loss: 0.000000)\n",
            "í•™ìŠµ ì§„í–‰ë¥ : 500/500 (Loss: 0.000000)\n",
            "ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n",
            "\n",
            "ìž…ë ¥ ë¬¸ìž¥                                    | íŒë‹¨ ê²°ê³¼ (Label)\n",
            "--------------------------------------------------------------------------------\n",
            "êµìˆ˜ë‹˜ ì•ˆë…•í•˜ì„¸ìš”, ê³¼ì œ ê´€ë ¨ ë¬¸ì˜ë“œë¦½ë‹ˆë‹¤.                 | Label 1 : ê²©ì‹ (êµìˆ˜ë‹˜ìš©)\n",
            "ì•¼ ë„ˆ ì˜¤ëŠ˜ í”¼ë°© ê°ˆëž˜?                            | Label 0 : ë¹„ê²©ì‹ (ì¹œêµ¬ìš©)\n",
            "í˜¹ì‹œ ëž©ë¯¸íŒ… ì¼ì •ì´ ë³€ê²½ë˜ì—ˆë‚˜ìš”?                       | Label 1 : ê²©ì‹ (êµìˆ˜ë‹˜ìš©)\n",
            "ì•„ ì§„ì§œ ê°œí”¼ê³¤í•˜ë‹¤ ã…‹ã…‹ã…‹                           | Label 0 : ë¹„ê²©ì‹ (ì¹œêµ¬ìš©)\n",
            "ì£„ì†¡í•˜ì§€ë§Œ ì¶œì„ ì¸ì • ì„œë¥˜ ì œì¶œì´ ëŠ¦ì–´ì§ˆ ê²ƒ ê°™ìŠµë‹ˆë‹¤.           | Label 1 : ê²©ì‹ (êµìˆ˜ë‹˜ìš©)\n"
          ]
        }
      ]
    }
  ]
}