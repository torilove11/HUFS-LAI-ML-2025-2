{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 드라이브 연동"
      ],
      "metadata": {
        "id": "J_IJ_rHZxKZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "%cd /content/drive/MyDrive/ML_Project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFOEddH0wtGF",
        "outputId": "efe2796b-6e33-4d94-e158-c980caabb94e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/ML_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 및 벡터 로드"
      ],
      "metadata": {
        "id": "Mr1HTajXxMCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Logistic Regression 최종 모델 로드\n",
        "model = joblib.load(\"final_model.pkl\")\n",
        "tfidf = joblib.load(\"tfidf_vectorizer.pkl\")\n",
        "\n",
        "print(\"모델 및 벡터 로드 완료\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lmaRXTdw2RE",
        "outputId": "cf01d07d-bc29-4d35-d704-97bbeb8839bd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델 및 벡터 로드 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리(기존과 동일)"
      ],
      "metadata": {
        "id": "ePL1ZE7FxPKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Training 때 사용한 stopword 리스트 그대로 복붙\n",
        "stopwords = set([\n",
        "    # 1) filler / 말버릇\n",
        "    \"어\", \"음\", \"자\", \"뭐\", \"요\", \"네\", \"막\", \"그냥\",\n",
        "    \"근데\", \"그죠\", \"거죠\", \"예\",\n",
        "\n",
        "    # 2) 지시어 (정보 없음)\n",
        "    \"이거\", \"이게\", \"이건\", \"그거\", \"그게\",\n",
        "    \"저거\", \"요거\", \"요게\", \"얘는\",\n",
        "\n",
        "    # 3) 공손/형식적 표현\n",
        "    \"합니다\", \"되었습니다\", \"됩니다\", \"하겠습니다\",\n",
        "    \"해주세요\", \"드릴\", \"보겠습니다\", \"할게요\",\n",
        "    \"있습니다\", \"있어요\", \"있죠\",\n",
        "\n",
        "    # 4) 기능어 (기본 조사)\n",
        "    \"이\", \"그\", \"저\", \"을\", \"를\", \"은\", \"는\", \"에\",\n",
        "    \"에서\", \"로\", \"것\", \"거\", \"건\", \"것들\",\n",
        "])\n",
        "\n",
        "def preprocess(text):\n",
        "    text = str(text).lower()                      # 소문자 변환\n",
        "    text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \" \", text)  # 한글/영문/숫자 제외 모두 제거\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()      # 중복 공백 제거\n",
        "\n",
        "    # 토큰화\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Stopword 제거\n",
        "    tokens = [t for t in tokens if t not in stopwords]\n",
        "\n",
        "    # Training과 완전히 동일하게 문장 재구성\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "Mn2TOlZ9w3mN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 단일 문장 추론"
      ],
      "metadata": {
        "id": "SIjv8R0qxR6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentence(sentence, threshold=0.06):\n",
        "    clean = preprocess(sentence)\n",
        "    vec = tfidf.transform([clean])\n",
        "    prob_1 = model.predict_proba(vec)[0][1]\n",
        "    pred = 1 if prob_1 >= threshold else 0\n",
        "    return pred, prob_1"
      ],
      "metadata": {
        "id": "t3t-tIN8w6LE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 여러 문장 리스트 추론"
      ],
      "metadata": {
        "id": "_QaSc0EgxTz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_batch(sentences, threshold=0.06):\n",
        "    clean_list = [preprocess(s) for s in sentences]\n",
        "    vec = tfidf.transform(clean_list)\n",
        "    prob_1 = model.predict_proba(vec)[:, 1]\n",
        "    preds = (prob_1 >= threshold).astype(int)\n",
        "    return preds, prob_1"
      ],
      "metadata": {
        "id": "uNUbZoJxw7_v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 테스트"
      ],
      "metadata": {
        "id": "W5wy7LkExXTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentences = [\n",
        "    \"이번 알고리즘의 핵심 아이디어는 데이터를 반복적으로 갱신하는 것입니다.\",\n",
        "    \"이 모델의 목적은 입력 문장에서 핵심 정보를 자동으로 추출하는 것입니다.\",\n",
        "    \"여기까지 첫 번째 파트 설명이었습니다.\",\n",
        "    \"혹시 질문 있으신가요?\"\n",
        "]\n",
        "\n",
        "preds, probs = predict_batch(example_sentences, threshold=0.06)\n",
        "\n",
        "for s, p, pr in zip(example_sentences, preds, probs):\n",
        "    print(f\"[문장] {s}\")\n",
        "    print(f\"=> 예측 라벨: {p} (확률: {pr:.4f})\")\n",
        "    print(\"   • 1 = 핵심 문장 / 0 = 비핵심 문장\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs8NYeKPw_Ad",
        "outputId": "63a2464d-effb-4ea6-e2b6-76696bd4cc64"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[문장] 이번 알고리즘의 핵심 아이디어는 데이터를 반복적으로 갱신하는 것입니다.\n",
            "=> 예측 라벨: 1 (확률: 0.1262)\n",
            "   • 1 = 핵심 문장 / 0 = 비핵심 문장\n",
            "\n",
            "[문장] 이 모델의 목적은 입력 문장에서 핵심 정보를 자동으로 추출하는 것입니다.\n",
            "=> 예측 라벨: 1 (확률: 0.0617)\n",
            "   • 1 = 핵심 문장 / 0 = 비핵심 문장\n",
            "\n",
            "[문장] 여기까지 첫 번째 파트 설명이었습니다.\n",
            "=> 예측 라벨: 0 (확률: 0.0329)\n",
            "   • 1 = 핵심 문장 / 0 = 비핵심 문장\n",
            "\n",
            "[문장] 혹시 질문 있으신가요?\n",
            "=> 예측 라벨: 0 (확률: 0.0080)\n",
            "   • 1 = 핵심 문장 / 0 = 비핵심 문장\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    s = input(\"문장을 입력하세요 (종료: quit): \")\n",
        "    if s.lower() == \"quit\":\n",
        "        break\n",
        "    pred, prob = predict_sentence(s)\n",
        "    print(f\"예측 라벨: {pred} (확률: {prob:.4f})  /  1=핵심, 0=비핵심\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqKiAoN0xA5B",
        "outputId": "6411c6ec-e805-499f-9bf2-71b8a93033c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문장을 입력하세요 (종료: quit): 분할 정복(Divide and Conquer) 알고리즘은 하나의 큰 문제를 작은 하위 문제로 분할(Divide)하고, 각 하위 문제를 해결(Conquer)한 뒤, 그 해결책들을 합쳐서(Combine) 원래의 문제를 해결하는 알고리즘 설계 기법입니다.\n",
            "예측 라벨: 1 (확률: 0.2749)  /  1=핵심, 0=비핵심\n",
            "\n",
            "문장을 입력하세요 (종료: quit): 여러분들 전부 이해하셨죠?\n",
            "예측 라벨: 0 (확률: 0.0502)  /  1=핵심, 0=비핵심\n",
            "\n",
            "문장을 입력하세요 (종료: quit): 아 네 그러면 여기까지 하겠습니다.\n",
            "예측 라벨: 0 (확률: 0.0254)  /  1=핵심, 0=비핵심\n",
            "\n",
            "문장을 입력하세요 (종료: quit): 잠시만요, 화면이 안 보이네요.\n",
            "예측 라벨: 0 (확률: 0.0081)  /  1=핵심, 0=비핵심\n",
            "\n",
            "문장을 입력하세요 (종료: quit): quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 핵심 문장 자동 추출기 (프로젝트 데모용)"
      ],
      "metadata": {
        "id": "jUK0GvH9Cf_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# 문장 단위로 자동 split\n",
        "def split_sentences(text):\n",
        "    # 마침표, 물음표, 느낌표 기준으로 문장 분리\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "    # 빈 문장 제거\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "# 긴 텍스트 한 번에 입력 → 문장 자동 분리 → 핵심만 추출\n",
        "def extract_key_from_text(threshold=0.06):\n",
        "    print(\"=\"*70)\n",
        "    print(\"강의자료 자동 요약 — 긴 텍스트 입력 모드\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"여러 문장을 한 번에 붙여넣으세요. (종료: 빈 줄 + Enter)\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    print(\"텍스트 입력 시작 ↓\")\n",
        "    text_lines = []\n",
        "    while True:\n",
        "        line = input()\n",
        "        if line.strip() == \"\":\n",
        "            break\n",
        "        text_lines.append(line)\n",
        "\n",
        "    full_text = \"\\n\".join(text_lines).strip()\n",
        "\n",
        "    if len(full_text) == 0:\n",
        "        print(\"입력된 텍스트가 없습니다.\")\n",
        "        return\n",
        "\n",
        "    # 문장 자동 분리\n",
        "    sentences = split_sentences(full_text)\n",
        "    print(f\"\\n총 {len(sentences)}개 문장으로 분리되었습니다.\")\n",
        "\n",
        "    preds, probs = predict_batch(sentences, threshold=threshold)\n",
        "\n",
        "    # 핵심 문장만 추출\n",
        "    key_sents = [(s, pr) for s, p, pr in zip(sentences, preds, probs) if p == 1]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"추출된 핵심 문장\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    if not key_sents:\n",
        "        print(\"핵심 문장이 없습니다. (threshold =\", threshold, \")\")\n",
        "        return\n",
        "\n",
        "    for idx, (s, pr) in enumerate(key_sents, 1):\n",
        "        print(f\"\\n[{idx}] {s}\")\n",
        "        print(f\"     ↳ 확률: {pr:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✔ 총\", len(key_sents), \"개의 핵심 문장을 자동으로 추출했습니다.\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "# 실행\n",
        "extract_key_from_text(threshold=0.06)"
      ],
      "metadata": {
        "id": "9KUeMwfoClPl",
        "outputId": "e020e73c-0e1c-4777-8120-4ad168b52e95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "강의자료 자동 요약 — 긴 텍스트 입력 모드\n",
            "======================================================================\n",
            "여러 문장을 한 번에 붙여넣으세요. (종료: 빈 줄 + Enter)\n",
            "----------------------------------------------------------------------\n",
            "텍스트 입력 시작 ↓\n",
            "오늘은 경사하강법(Gradient Descent)의 핵심 아이디어를 살펴보겠습니다. 경사하강법은 비용 함수를 최소화하기 위해 기울기 방향으로 파라미터를 갱신하는 최적화 알고리즘입니다. 즉, 기울기가 큰 방향은 손실이 빠르게 감소하는 방향을 의미합니다. 이 과정에서 학습률(learning rate)은 이동하는 보폭을 조절하는 중요한 하이퍼파라미터입니다. 학습률이 너무 크면 발산할 수 있고, 너무 작으면 학습 속도가 매우 느려집니다. 여기서 혹시 질문 있으신가요? 자 그럼 간단한 예제를 보면서 이해해봅시다. 다음 그래프는 비용 함수의 곡선을 나타냅니다. 우리가 원하는 것은 이 곡선의 최솟값을 찾는 것입니다.\n",
            "\n",
            "\n",
            "총 9개 문장으로 분리되었습니다.\n",
            "\n",
            "======================================================================\n",
            "추출된 핵심 문장\n",
            "======================================================================\n",
            "\n",
            "[1] 오늘은 경사하강법(Gradient Descent)의 핵심 아이디어를 살펴보겠습니다.\n",
            "     ↳ 확률: 0.2866\n",
            "\n",
            "[2] 이 과정에서 학습률(learning rate)은 이동하는 보폭을 조절하는 중요한 하이퍼파라미터입니다.\n",
            "     ↳ 확률: 0.0977\n",
            "\n",
            "[3] 학습률이 너무 크면 발산할 수 있고, 너무 작으면 학습 속도가 매우 느려집니다.\n",
            "     ↳ 확률: 0.0620\n",
            "\n",
            "[4] 우리가 원하는 것은 이 곡선의 최솟값을 찾는 것입니다.\n",
            "     ↳ 확률: 0.2284\n",
            "\n",
            "======================================================================\n",
            "✔ 총 4 개의 핵심 문장을 자동으로 추출했습니다.\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 데모 셀에서 입력한 문장들:\n",
        "\n",
        "오늘은 경사하강법(Gradient Descent)의 핵심 아이디어를 살펴보겠습니다.\n",
        "경사하강법은 비용 함수를 최소화하기 위해 기울기 방향으로 파라미터를 갱신하는 최적화 알고리즘입니다.\n",
        "즉, 기울기가 큰 방향은 손실이 빠르게 감소하는 방향을 의미합니다.\n",
        "이 과정에서 학습률(learning rate)은 이동하는 보폭을 조절하는 중요한 하이퍼파라미터입니다.\n",
        "학습률이 너무 크면 발산할 수 있고, 너무 작으면 학습 속도가 매우 느려집니다.\n",
        "여기서 혹시 질문 있으신가요?\n",
        "자 그럼 간단한 예제를 보면서 이해해봅시다.\n",
        "다음 그래프는 비용 함수의 곡선을 나타냅니다.\n",
        "우리가 원하는 것은 이 곡선의 최솟값을 찾는 것입니다.\n",
        "\n",
        "내가 생각하는 핵심문장:\n",
        "\n",
        "1. 경사하강법은 비용 함수를 최소화하기 위해 기울기 방향으로 파라미터를 갱신하는 최적화 알고리즘입니다.(불일치)\n",
        "\n",
        "2. 즉, 기울기가 큰 방향은 손실이 빠르게 감소하는 방향을 의미합니다.(불일치)\n",
        "\n",
        "3. 이 과정에서 학습률(learning rate)은 이동하는 보폭을 조절하는 중요한 하이퍼파라미터입니다.(일치)\n",
        "\n",
        "4. 학습률이 너무 크면 발산할 수 있고, 너무 작으면 학습 속도가 매우 느려집니다.(일치)\n",
        "\n",
        "-> 약 50% 일치"
      ],
      "metadata": {
        "id": "OMnazoVGFGgu"
      }
    }
  ]
}