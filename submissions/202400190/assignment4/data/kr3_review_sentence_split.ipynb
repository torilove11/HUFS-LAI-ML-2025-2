{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Korean Sentence Splitter(kss) 설치"
      ],
      "metadata": {
        "id": "2r3vkTvwFX68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dnoVSdFBx_W",
        "outputId": "c0405199-4d17-47cd-d115-9003a3c22698"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kss\n",
            "  Downloading kss-6.0.6.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting emoji==1.2.0 (from kss)\n",
            "  Downloading emoji-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pecab (from kss)\n",
            "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from kss) (3.5)\n",
            "Collecting jamo (from kss)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting hangul-jamo (from kss)\n",
            "  Downloading hangul_jamo-1.0.1-py3-none-any.whl.metadata (899 bytes)\n",
            "Collecting tossi (from kss)\n",
            "  Downloading tossi-0.3.1.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting distance (from kss)\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from kss) (6.0.3)\n",
            "Collecting unidecode (from kss)\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting cmudict (from kss)\n",
            "  Downloading cmudict-1.1.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting koparadigm (from kss)\n",
            "  Downloading koparadigm-0.10.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting kollocate (from kss)\n",
            "  Downloading kollocate-0.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting bs4 (from kss)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from kss) (2.0.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from kss) (8.4.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from kss) (1.16.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from bs4->kss) (4.13.5)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.12/dist-packages (from cmudict->kss) (8.7.0)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.12/dist-packages (from cmudict->kss) (6.5.2)\n",
            "Collecting whoosh (from kollocate->kss)\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting xlrd==1.2.0 (from koparadigm->kss)\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from pecab->kss) (18.1.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from pecab->kss) (2024.11.6)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest->kss) (2.3.0)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from pytest->kss) (25.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest->kss) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest->kss) (2.19.2)\n",
            "Collecting bidict (from tossi->kss)\n",
            "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from tossi->kss) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=5->cmudict->kss) (3.23.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4->kss) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4->kss) (4.15.0)\n",
            "Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading cmudict-1.1.2-py3-none-any.whl (939 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.7/939.7 kB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hangul_jamo-1.0.1-py3-none-any.whl (4.4 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading kollocate-0.0.2-py3-none-any.whl (72.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading koparadigm-0.10.0-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
            "Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: kss, distance, pecab, tossi\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-6.0.6-cp312-cp312-linux_x86_64.whl size=1461883 sha256=9fff713332c6c9d318bbe22fd2ce06d69a33f05dd08e6d1564b858c63ec0f24f\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/ae/78/a246097c6733b18c9fa2ad07d434a1049c0fcae636266775d7\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16256 sha256=5566c91c03274d09844d3b755366ee1c2742986c857f4d4f99a7cdbe6dd7105e\n",
            "  Stored in directory: /root/.cache/pip/wheels/24/a8/58/407063d8e5c1d4dd6594c99d12baa0108570b56a92325587dd\n",
            "  Building wheel for pecab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646664 sha256=d158e4c387ce07754fe660a290f762d33d3dd38ac860af281c6bb5acd99c57e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/06/66/408e8d59b0d07d3f8c8c82d35641ec67f3fbdf79ebaa0815cd\n",
            "  Building wheel for tossi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tossi: filename=tossi-0.3.1-py3-none-any.whl size=12129 sha256=bf51c1437094de684160e6378a4b5cb3f3441be230c3291383d85bf800e1955d\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/2d/a2/9621c908a0b81992c5357c8100dee0d3cbeeba0398a2d1a883\n",
            "Successfully built kss distance pecab tossi\n",
            "Installing collected packages: whoosh, jamo, hangul-jamo, emoji, distance, xlrd, unidecode, kollocate, bidict, tossi, pecab, koparadigm, cmudict, bs4, kss\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 2.0.2\n",
            "    Uninstalling xlrd-2.0.2:\n",
            "      Successfully uninstalled xlrd-2.0.2\n",
            "Successfully installed bidict-0.23.1 bs4-0.0.2 cmudict-1.1.2 distance-0.1.3 emoji-1.2.0 hangul-jamo-1.0.1 jamo-0.4.1 kollocate-0.0.2 koparadigm-0.10.0 kss-6.0.6 pecab-1.0.8 tossi-0.3.1 unidecode-1.4.0 whoosh-2.7.4 xlrd-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!file /content/kr3.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsfIYucmDcTI",
        "outputId": "7cc332d3-30ac-4451-dbd1-5322eb4a9ccb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kr3.tsv: cannot open `/content/kr3.tsv' (No such file or directory)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "원본 데이터셋(utf-8) 불러오기"
      ],
      "metadata": {
        "id": "FGLYaoWkTfck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import kss\n",
        "\n",
        "# 1.\n",
        "df = pd.read_csv(\n",
        "    \"/content/kr3_utf8.csv\",  # tsv 파일 이름\n",
        "    encoding=\"utf-8\",\n",
        "    engine=\"python\",\n",
        "    on_bad_lines=\"skip\",\n",
        ")\n",
        "\n",
        "print(df.head())\n",
        "print(df.columns)\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4f4liXnSlr_",
        "outputId": "f67e0ceb-32bf-4b10-d289-32971be2ebde"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Rating                                             Review\n",
            "0       1  숙성 돼지고기 전문점입니다. 건물 모양 때문에 매장 모양도 좀 특이하지만 쾌적한 편...\n",
            "1       1  고기가 정말 맛있었어요! 육즙이 가득 있어서 너무 좋았아요 일하시는 분들 너무 친절...\n",
            "2       1  잡내 없고 깔끔, 담백한 맛의 순댓국이 순댓국을 안 좋아하는 사람들에게도 술술 넘어...\n",
            "3       1  고기 양이 푸짐해서 특 순대국밥을 시킨 기분이 듭니다 맛도 좋습니다 다만 양념장이 ...\n",
            "4       1  순댓국 자체는 제가 먹어본 순대국밥집 중에서 Top5 안에는 들어요. 그러나 밥 양...\n",
            "Index(['Rating', 'Review'], dtype='object')\n",
            "453798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentence split labeling template 파일 생성(csv, utf-8)\n"
      ],
      "metadata": {
        "id": "sx1r4CKkTjtv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c9JYf9IA80c",
        "outputId": "74a63633-de8e-4f46-db18-394d702e05e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pecab/_tokenizer.py:265: RuntimeWarning: overflow encountered in scalar add\n",
            "  from_pos_data.costs[idx]\n",
            "/usr/local/lib/python3.12/dist-packages/pecab/_tokenizer.py:274: RuntimeWarning: overflow encountered in scalar add\n",
            "  least_cost += word_cost\n"
          ]
        }
      ],
      "source": [
        "# 2. Rating이 0/1인 것만 사용 (2는 완전히 제외)\n",
        "df = df[df[\"Rating\"].isin([0, 1])].copy()\n",
        "\n",
        "# 3. doc_label을 0/1로 그대로 사용\n",
        "df[\"doc_label\"] = df[\"Rating\"].astype(int)\n",
        "\n",
        "# 4. 긍정/부정에서 원하는 개수 샘플링\n",
        "n_pos = 100\n",
        "n_neg = 100\n",
        "\n",
        "pos_df = df[df[\"Rating\"] == 1].sample(\n",
        "    n=min(n_pos, (df[\"Rating\"] == 1).sum()),\n",
        "    random_state=42,\n",
        ")\n",
        "neg_df = df[df[\"Rating\"] == 0].sample(\n",
        "    n=min(n_neg, (df[\"Rating\"] == 0).sum()),\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "sample_df = pd.concat([pos_df, neg_df], ignore_index=True)\n",
        "\n",
        "# 5. review_id 부여 (1, 2, 3, ...)\n",
        "sample_df = sample_df.reset_index(drop=True)\n",
        "sample_df[\"review_id\"] = sample_df.index + 1\n",
        "\n",
        "# 6. 문장 단위로 쪼개서 템플릿 만들기\n",
        "rows = []\n",
        "\n",
        "for _, row in sample_df.iterrows():\n",
        "    review_id = int(row[\"review_id\"])\n",
        "    doc_label = int(row[\"doc_label\"])      # 0 또는 1\n",
        "    text = str(row[\"Review\"])              # 엑셀에서 Review라는 컬럼명\n",
        "\n",
        "    # Korean Sentence Splitter(kss)로 한국어 문장 분리\n",
        "    sentences = kss.split_sentences(text)\n",
        "\n",
        "    for sent_id, sent in enumerate(sentences, start=1):\n",
        "        rows.append({\n",
        "            \"review_id\": review_id,\n",
        "            \"doc_label\": doc_label,        # 0/1\n",
        "            \"sentence_id\": sent_id,\n",
        "            \"sentence_text\": sent.strip(),\n",
        "            \"sent_label\": \"\"               # 여기다가 나중에 직접 라벨링\n",
        "        })\n",
        "\n",
        "sent_df = pd.DataFrame(rows)\n",
        "\n",
        "# 7.\n",
        "sent_df.to_csv(\n",
        "    \"/content/kr3_sentence_label.csv\",\n",
        "    index=False,\n",
        "    encoding=\"utf-8-sig\",               # 엑셀 친화적인 UTF-8 + BOM\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "리뷰 개수 추가(POS/NEG 각 100개씩)"
      ],
      "metadata": {
        "id": "e9In_RLjrs13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import kss\n",
        "\n",
        "DOC_CSV  = \"/content/kr3_utf8.csv\"\n",
        "SENT_CSV = \"/content/kr3_sentence_label.csv\"\n",
        "\n",
        "# 1. 원본 KR3 읽기\n",
        "df = pd.read_csv(DOC_CSV, encoding=\"utf-8\", engine=\"python\", on_bad_lines=\"skip\")\n",
        "df = df[df[\"Rating\"].isin([0, 1])].copy()\n",
        "df[\"doc_label\"] = df[\"Rating\"].astype(int)\n",
        "df = df.reset_index(drop=False).rename(columns={\"index\": \"doc_idx\"})  # 각 리뷰의 고유 idx\n",
        "\n",
        "# 2. 기존 sentence CSV 읽기 (없으면 빈 df)\n",
        "try:\n",
        "    sent_df = pd.read_csv(SENT_CSV, encoding=\"utf-8\")\n",
        "except FileNotFoundError:\n",
        "    sent_df = pd.DataFrame(columns=[\n",
        "        \"review_id\", \"doc_label\", \"sentence_id\", \"sentence_text\", \"sent_label\", \"source_doc_idx\"\n",
        "    ])\n",
        "\n",
        "if \"source_doc_idx\" not in sent_df.columns:\n",
        "    sent_df[\"source_doc_idx\"] = pd.NA\n",
        "\n",
        "# 3. 이미 쓴 리뷰(doc_idx) 제외\n",
        "used_doc_idx = set(sent_df[\"source_doc_idx\"].dropna().astype(int).unique())\n",
        "unused_df = df[~df[\"doc_idx\"].isin(used_doc_idx)].copy()\n",
        "\n",
        "# 새로 추가할 리뷰 개수\n",
        "n_new_pos = 100\n",
        "n_new_neg = 100\n",
        "\n",
        "pos_df = unused_df[unused_df[\"Rating\"] == 1].sample(\n",
        "    n=min(n_new_pos, (unused_df[\"Rating\"] == 1).sum()),\n",
        "    random_state=42,\n",
        ")\n",
        "neg_df = unused_df[unused_df[\"Rating\"] == 0].sample(\n",
        "    n=min(n_new_neg, (unused_df[\"Rating\"] == 0).sum()),\n",
        "    random_state=42,\n",
        ")\n",
        "sample_df = pd.concat([pos_df, neg_df], ignore_index=True)\n",
        "\n",
        "# 4. review_id는 기존 max 다음부터\n",
        "if len(sent_df) == 0:\n",
        "    next_review_id = 1\n",
        "else:\n",
        "    next_review_id = int(sent_df[\"review_id\"].max()) + 1\n",
        "\n",
        "rows = []\n",
        "for _, row in sample_df.iterrows():\n",
        "    review_id = next_review_id\n",
        "    next_review_id += 1\n",
        "\n",
        "    doc_label = int(row[\"doc_label\"])\n",
        "    text = str(row[\"Review\"])\n",
        "    doc_idx = int(row[\"doc_idx\"])\n",
        "\n",
        "    sentences = kss.split_sentences(text)\n",
        "    for sent_id, sent in enumerate(sentences, start=1):\n",
        "        rows.append({\n",
        "            \"review_id\": review_id,\n",
        "            \"doc_label\": doc_label,\n",
        "            \"sentence_id\": sent_id,\n",
        "            \"sentence_text\": sent.strip(),\n",
        "            \"sent_label\": \"\",\n",
        "            \"source_doc_idx\": doc_idx,\n",
        "        })\n",
        "\n",
        "new_sent_df = pd.DataFrame(rows)\n",
        "\n",
        "# 5. 기존 + 신규 합쳐서 다시 저장 (append 효과)\n",
        "final_df = pd.concat([sent_df, new_sent_df], ignore_index=True)\n",
        "final_df.to_csv(SENT_CSV, index=False, encoding=\"utf-8-sig\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhvzVcZ2ruEH",
        "outputId": "ee4b22b0-e7be-4320-d0f9-acdc2ae42c88"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pecab/_tokenizer.py:265: RuntimeWarning: overflow encountered in scalar add\n",
            "  from_pos_data.costs[idx]\n",
            "/usr/local/lib/python3.12/dist-packages/pecab/_tokenizer.py:274: RuntimeWarning: overflow encountered in scalar add\n",
            "  least_cost += word_cost\n"
          ]
        }
      ]
    }
  ]
}