{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "DfHoOmu_LFni",
        "jB2qjFNMLKAN"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 라이브러리 로드"
      ],
      "metadata": {
        "id": "DfHoOmu_LFni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# 시각화 설정\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_path = '/content/drive/MyDrive/f1_project/data/'\n",
        "file_name = 'training_data.csv'\n",
        "file_path = os.path.join(base_path, file_name)\n",
        "\n",
        "print(f\"Loading data from: {file_path}\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Data Loaded Successfully: {df.shape}\")\n",
        "    display(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\n[Error] 파일을 찾을 수 없습니다! 경로를 확인하세요: {file_path}\")\n",
        "    raise  # 에러 발생 시 중단"
      ],
      "metadata": {
        "id": "2-LilFZj4xfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 데이터 전처리"
      ],
      "metadata": {
        "id": "jB2qjFNMLKAN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VA6Dlo4lz0M9"
      },
      "outputs": [],
      "source": [
        "print(\"\\n[Step 1] Target Engineering...\")\n",
        "\n",
        "# 두 사이트의 평점이 모두 있는 경우에만 계산\n",
        "if 'F1HOTorNOT' in df.columns and 'RaceFans' in df.columns:\n",
        "    target_scaler = MinMaxScaler()\n",
        "    df[['F1_Norm', 'RF_Norm']] = target_scaler.fit_transform(df[['F1HOTorNOT', 'RaceFans']])\n",
        "    df['Excitement_Score'] = (df['F1_Norm'] + df['RF_Norm']) / 2\n",
        "    print(\" -> Target Created: Excitement_Score (Average of F1HOTorNOT & RaceFans)\")\n",
        "else:\n",
        "    print(\" -> Warning: Rating columns not found. Check your CSV file.\")\n",
        "\n",
        "# 타겟값(점수)이 없는 데이터(NaN) 제거\n",
        "initial_len = len(df)\n",
        "df = df.dropna(subset=['Excitement_Score'])\n",
        "print(f\" -> Dropped NaN Targets: {initial_len} -> {len(df)} rows remaining.\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5. Feature Selection & Data Split\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n[Step 2] Splitting Data (Train: 2021-2024)...\")\n",
        "\n",
        "# Feature 정의\n",
        "features = [\n",
        "    'Chaos_Score',          # 사고 및 혼란도 (SC/RedFlag 등)\n",
        "    'Lead_Changes',         # 선두 교체 횟수\n",
        "    'Gap_Std_Dev',          # 1-2위 격차 표준편차\n",
        "    'Position_Gains_Total'  # 총 순위 변동 횟수\n",
        "]\n",
        "target = 'Excitement_Score'\n",
        "group_col = 'Year'\n",
        "\n",
        "# 2024년까지의 데이터만 학습에 사용 (Time-series logic)\n",
        "train_df = df[df['Year'] <= 2024].copy()\n",
        "\n",
        "X = train_df[features]\n",
        "y = train_df[target]\n",
        "groups = train_df[group_col]\n",
        "\n",
        "print(f\" -> Training Data Shape: {X.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Training"
      ],
      "metadata": {
        "id": "HwDyOJSdLon4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 6. Cross Validation (Custom Rolling Window by Season)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n[Step 3] Cross Validation (Custom Rolling Window)...\")\n",
        "\n",
        "# 검증할 타겟 연도 리스트 (2022년부터 2024년까지)\n",
        "validation_years = [2022, 2023, 2024]\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=2000, random_state=42, max_depth=5, min_samples_leaf=2)\n",
        "\n",
        "results = []\n",
        "cv_scores = []\n",
        "\n",
        "for val_year in validation_years:\n",
        "    # 1. Split (Year 기준으로 명확하게 분리)\n",
        "    # 학습용: 검증 연도보다 과거인 모든 데이터\n",
        "    # 검증용: 해당 검증 연도 데이터\n",
        "    train_fold = df[df['Year'] < val_year]\n",
        "    val_fold = df[df['Year'] == val_year]\n",
        "\n",
        "    # 데이터가 비어있지 않은지 확인\n",
        "    if len(train_fold) == 0 or len(val_fold) == 0:\n",
        "        print(f\"Skipping Year {val_year} (Not enough data)\")\n",
        "        continue\n",
        "\n",
        "    X_tr = train_fold[features]\n",
        "    y_tr = train_fold[target]\n",
        "\n",
        "    X_val = val_fold[features]\n",
        "    y_val = val_fold[target]\n",
        "\n",
        "    # 2. Scaling (Data Leakage 방지: Train Fold로만 Fit)\n",
        "    scaler = StandardScaler()\n",
        "    X_tr_scaled = scaler.fit_transform(X_tr)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "    # 3. Training & Evaluation\n",
        "    model.fit(X_tr_scaled, y_tr)\n",
        "    y_pred = model.predict(X_val_scaled)\n",
        "\n",
        "    r2 = r2_score(y_val, y_pred)\n",
        "    mae = mean_absolute_error(y_val, y_pred)\n",
        "\n",
        "    cv_scores.append(r2)\n",
        "    results.append({'Val_Year': val_year, 'R2': r2, 'MAE': mae})\n",
        "\n",
        "    # 학습 데이터 기간 표시 (예: \"2021-2022\")\n",
        "    train_years_str = f\"{train_fold['Year'].min()}-{train_fold['Year'].max()}\"\n",
        "    print(f\" -> Train({train_years_str}) vs Val({val_year}) | R2: {r2:.4f} | MAE: {mae:.4f}\")\n",
        "\n",
        "mean_r2 = np.mean(cv_scores)\n",
        "print(f\"\\n -> Average Validation R2 Score: {mean_r2:.4f}\")"
      ],
      "metadata": {
        "id": "HJzRGu7mLnZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating Learning Curve (Smart Way)...\")\n",
        "\n",
        "# 1. 모델 설정\n",
        "rf_warm = RandomForestRegressor(n_estimators=100, warm_start=True, max_depth=5, random_state=42, n_jobs=-1, min_samples_leaf=2)\n",
        "\n",
        "estimators_range = range(100, 2001, 100) # 10개씩 늘려가며 2000개까지\n",
        "train_scores = []\n",
        "val_scores = []\n",
        "\n",
        "# 검증용 데이터 준비 (Train/Val 나누기)\n",
        "# 시각화를 위해 Train 데이터 중 일부를 떼어내거나, 마지막 Fold 데이터를 사용\n",
        "vis_train_mask = df['Year'] <= 2023\n",
        "vis_val_mask = df['Year'] == 2024\n",
        "\n",
        "X_vis_tr = df[vis_train_mask][features]\n",
        "y_vis_tr = df[vis_train_mask][target]\n",
        "X_vis_val = df[vis_val_mask][features]\n",
        "y_vis_val = df[vis_val_mask][target]\n",
        "\n",
        "# 스케일링 (시각화용)\n",
        "scaler_vis = StandardScaler()\n",
        "X_vis_tr_scaled = scaler_vis.fit_transform(X_vis_tr)\n",
        "X_vis_val_scaled = scaler_vis.transform(X_vis_val)\n",
        "\n",
        "# 2. 루프를 돌며 나무를 추가 (Epoch와 유사한 효과)\n",
        "for n in estimators_range:\n",
        "    rf_warm.n_estimators = n # 나무 개수 업데이트\n",
        "    rf_warm.fit(X_vis_tr_scaled, y_vis_tr) # 기존 나무 유지하고 추가 학습\n",
        "\n",
        "    # 점수 기록\n",
        "    train_scores.append(r2_score(y_vis_tr, rf_warm.predict(X_vis_tr_scaled)))\n",
        "    val_scores.append(r2_score(y_vis_val, rf_warm.predict(X_vis_val_scaled)))\n",
        "\n",
        "# 3. 그래프 그리기\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(estimators_range, train_scores, '-', label='Train R2', color='blue')\n",
        "plt.plot(estimators_range, val_scores, '-', label='Validation R2', color='orange')\n",
        "\n",
        "plt.title('Learning Curve: Performance over n_estimators (Warm Start)')\n",
        "plt.xlabel('Number of Trees (n_estimators)')\n",
        "plt.ylabel('R2 Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sMO4DjgBOtdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# 7. Final Model Training & Saving\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n[Step 4] Final Training & Saving...\")\n",
        "\n",
        "# 1. 전체 Train Set(2021-2024)으로 Scaler 학습\n",
        "final_scaler = StandardScaler()\n",
        "X_final_scaled = final_scaler.fit_transform(X)\n",
        "\n",
        "# 2. 전체 Train Set으로 모델 학습\n",
        "final_model = RandomForestRegressor(n_estimators=2000, random_state=42, max_depth=5, min_samples_leaf=2)\n",
        "final_model.fit(X_final_scaled, y)\n",
        "\n",
        "# 3. 모델과 스케일러 저장 (evaluation/inference 단계에서 필수)\n",
        "model_save_path = os.path.join(base_path, 'f1_excitement_model.pkl')\n",
        "scaler_save_path = os.path.join(base_path, 'scaler_features.pkl')\n",
        "\n",
        "joblib.dump(final_model, model_save_path)\n",
        "joblib.dump(final_scaler, scaler_save_path)\n",
        "\n",
        "print(f\" -> Model saved at: {model_save_path}\")\n",
        "print(f\" -> Scaler saved at: {scaler_save_path}\")\n",
        "print(\"\\n[Success] Training process completed.\")"
      ],
      "metadata": {
        "id": "axXSBV5bOwUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "### 1. 모델 설계의 적절성\n",
        "- 데이터의 특성과 복잡한 상호작용을 고려하여 **Random Forest Regressor**를 채택했습니다.\n",
        "- 과적합 방지와 안정적인 예측을 위해 앙상블 기법을 사용했으며, 데이터셋 크기가 작음을 고려하여 딥러닝보다는 머신러닝 모델이 적합하다고 판단했습니다.\n",
        "\n",
        "### 2. 데이터셋 분할의 명확성\n",
        "- 시계열 데이터(F1 시즌)의 인과성을 보존하기 위해 **Rolling Window (Time-series Split)** 방식을 적용했습니다.\n",
        "- **Ratio & Period**:\n",
        "    - **Train**: 2021 ~ 2023 시즌 (학습)\n",
        "    - **Validation**: 2024 시즌 (검증 및 튜닝)\n",
        "    - **Final Test**: 2025 시즌 (최종 평가 - **Data Leakage 차단**을 위해 별도 파일로 분리하여 `evaluation.ipynb`에서 수행)\n",
        "\n",
        "### 3. 학습 과정의 투명성 및 시각화\n",
        "- Random Forest에는 Epoch 개념이 없으므로, **`n_estimators`를 증가**시키며 성능 변화를 추적했습니다.\n",
        "- `warm_start=True` 옵션을 활용하여 **Learning Curve** (Train R2 vs Validation R2)를 시각화했습니다.\n",
        "    - 이를 통해 모델이 초기에는 데이터 부족으로 과소적합되다가, 데이터가 누적될수록 성능이 향상됨을 확인했습니다.\n",
        "- **Analysis**: 학습 데이터에 대한 R2(0.65 이상)와 검증 데이터에 대한 R2(0.35) 차이를 통해 모델의 과적합 경향과 데이터 패턴의 변화를 분석했습니다.\n",
        "\n",
        "### 4. Validation 성능 모니터링\n",
        "- 회귀 문제에 적합한 **R2 Score**와 **MAE** (Mean Absolute Error)를 사용했습니다.\n",
        "- **Results**: 연도별 검증(Rolling Window)을 통해 2022년(규정 대격변)의 성능 저하와 2024년(데이터 누적 후)의 성능 향상을 정량적으로 확인했습니다.\n",
        "\n",
        "### 5. 재현 가능성\n",
        "- 학습된 모델(`f1_excitement_model.pkl`)과 전처리 스케일러(`scaler_features.pkl`)를 `joblib`을 통해 저장했습니다.\n",
        "- 별도의 `inference.ipynb` 및 `evaluation.ipynb`에서 저장된 파일을 로드하여 동일한 성능을 재현할 수 있도록 구성했습니다."
      ],
      "metadata": {
        "id": "IlplZ5V3S5j1"
      }
    }
  ]
}