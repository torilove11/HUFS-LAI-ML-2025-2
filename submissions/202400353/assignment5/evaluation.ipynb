{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ"
      ],
      "metadata": {
        "id": "ML7n80rvWzFo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqcyqNyVVpPB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# ì‹œê°í™” ì„¤ì •\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_path = '/content/drive/MyDrive/f1_project/data/'\n",
        "model_path = os.path.join(base_path, 'f1_excitement_model.pkl')\n",
        "scaler_path = os.path.join(base_path, 'scaler_features.pkl')\n",
        "\n",
        "test_file_name = 'finaltest_data.csv'\n",
        "test_file_path = os.path.join(base_path, test_file_name)\n",
        "print(f\"Loading Test Data from: {test_file_path}\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(test_file_path)\n",
        "    print(f\"Data Loaded Successfully: {df.shape}\")\n",
        "    display(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\n[Error] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {file_path}\")\n",
        "    raise  # ì—ëŸ¬ ë°œìƒ ì‹œ ì¤‘ë‹¨"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ëª¨ë¸ ë¡œë“œ, ë°ì´í„° ì „ì²˜ë¦¬"
      ],
      "metadata": {
        "id": "XW3A_8A5YRkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ë°ì´í„° ë¡œë“œ\n",
        "try:\n",
        "    model = joblib.load(model_path)\n",
        "    scaler = joblib.load(scaler_path)\n",
        "    print(\"âœ… Model & Scaler loaded successfully.\")\n",
        "\n",
        "    df_test = pd.read_csv(test_file_path)\n",
        "    print(f\"âœ… Data loaded. Shape: {df_test.shape}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}\")\n",
        "    print(\"training.ipynbë¥¼ ë¨¼ì € ì‹¤í–‰í–ˆëŠ”ì§€, csv íŒŒì¼ëª…ì´ ë§ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "    raise\n",
        "\n",
        "# 3. Target Engineering (MyScore -> Excitement_Score ë³€í™˜)\n",
        "# 10ì  ë§Œì ì„ ëª¨ë¸ ê¸°ì¤€ì¸ 0~1 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "if 'MyScore' in df_test.columns:\n",
        "    print(\"âœ… 'MyScore' column found. Converting to normalized Excitement_Score...\")\n",
        "    df_test['Excitement_Score'] = df_test['MyScore'] / 10.0\n",
        "elif 'Excitement_Score' in df_test.columns:\n",
        "    print(\"âœ… 'Excitement_Score' already exists.\")\n",
        "else:\n",
        "    raise ValueError(\"CSV íŒŒì¼ì— 'MyScore' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤! ì§ì ‘ ë§¤ê¸´ ì ìˆ˜ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "# ê²°ì¸¡ì¹˜ ì œê±° (ì ìˆ˜ë¥¼ ë§¤ê¸°ì§€ ì•Šì€ ë¯¸ë˜ ê²½ê¸°ëŠ” í‰ê°€ ì œì™¸)\n",
        "initial_len = len(df_test)\n",
        "df_test = df_test.dropna(subset=['Excitement_Score'])\n",
        "print(f\" -> Dropped NaN Targets: {initial_len} -> {len(df_test)} rows remaining.\")"
      ],
      "metadata": {
        "id": "8JCHauQTYRDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### í‰ê°€ ê³¼ì •"
      ],
      "metadata": {
        "id": "HBRaKJ7oYgJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. ì˜ˆì¸¡ ì‹¤í–‰\n",
        "features = ['Chaos_Score', 'Lead_Changes', 'Gap_Std_Dev', 'Position_Gains_Total']\n",
        "\n",
        "# Feature ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ì²´í¬\n",
        "if not all(col in df_test.columns for col in features):\n",
        "    missing = [col for col in features if col not in df_test.columns]\n",
        "    raise ValueError(f\"í…ŒìŠ¤íŠ¸ íŒŒì¼ì— ë‹¤ìŒ Feature ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing}\")\n",
        "\n",
        "X_test = df_test[features]\n",
        "y_true = df_test['Excitement_Score']\n",
        "\n",
        "# ì €ì¥ëœ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ Transform\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# =============================================================================\n",
        "# 5. ìµœì¢… ì„±ëŠ¥ í‰ê°€\n",
        "# =============================================================================\n",
        "\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred)) # RMSEë„ ì¶”ê°€í•˜ë©´ ë” ì „ë¬¸ì ìœ¼ë¡œ ë³´ì„\n",
        "\n",
        "# [í•µì‹¬ ë³´ì™„] ì£¼ìš” ì„±ëŠ¥ ìˆ˜ì¹˜ë¥¼ 'í‘œ(Table)' í˜•íƒœë¡œ ì •ë¦¬\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Metric': ['MAE (Mean Absolute Error)', 'RMSE (Root Mean Squared Error)', 'R2 Score'],\n",
        "    'Value': [mae, rmse, r2],\n",
        "    'Interpretation': [\n",
        "        f'í‰ê·  {mae*10:.2f}ì  ì˜¤ì°¨ (10ì  ë§Œì  ê¸°ì¤€)',\n",
        "        f'ì˜¤ì°¨ì˜ í‘œì¤€í¸ì°¨: {rmse*10:.2f}ì ',\n",
        "        f'ë°ì´í„° ì„¤ëª…ë ¥: {r2*100:.1f}%'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"ğŸ Final Test Performance Summary (2025 Season)\")\n",
        "print(\"=\"*50)\n",
        "display(metrics_df) # í‘œ í˜•íƒœë¡œ ê¹”ë”í•˜ê²Œ ì¶œë ¥ë¨\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "ax_erpDEYjDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì‹œê°í™”"
      ],
      "metadata": {
        "id": "YCRveDEAYojK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. ì‹œê°í™”: Actual vs Predicted Scatter Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# ì‚°ì ë„ ê·¸ë¦¬ê¸°\n",
        "sns.scatterplot(x=y_true, y=y_pred, s=150, color='royalblue', alpha=0.8, label='Race Data')\n",
        "\n",
        "# ê¸°ì¤€ì„  (Perfect Prediction Line)\n",
        "plt.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Ideal Fit (y=x)')\n",
        "\n",
        "# ê° ì ì— ë¼ìš´ë“œ ì´ë¦„ ë‹¬ì•„ì£¼ê¸° (ì–´ë–¤ ê²½ê¸°ë¥¼ ì˜ ë§ì·„ëŠ”ì§€ í™•ì¸ìš©)\n",
        "if 'GrandPrix' in df_test.columns:\n",
        "    for i in range(len(df_test)):\n",
        "        plt.text(y_true.iloc[i]+0.01, y_pred[i], df_test['GrandPrix'].iloc[i], fontsize=9, alpha=0.8)\n",
        "\n",
        "plt.xlabel('My Manual Score (Normalized 0~1)', fontsize=12)\n",
        "plt.ylabel('AI Model Prediction (0~1)', fontsize=12)\n",
        "plt.title('2025 Final Evaluation: My Ratings vs AI Predictions', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "result_df = df_test[['GrandPrix', 'MyScore', 'Excitement_Score']].copy()\n",
        "result_df['AI_Pred_Norm'] = y_pred\n",
        "result_df['AI_Pred_Score'] = y_pred * 10.0 # 10ì  ë§Œì ìœ¼ë¡œ í™˜ì‚°\n",
        "result_df['Diff'] = abs(result_df['MyScore'] - result_df['AI_Pred_Score'])\n",
        "\n",
        "print(\"\\n[Detailed Prediction Results]\")\n",
        "display(result_df.sort_values('Diff')) # ì˜¤ì°¨ê°€ ì ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬"
      ],
      "metadata": {
        "id": "bfW6h7hwYqD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}